# Airflow + PySpark + DynamoDB ETL Project

This project demonstrates how to build an end-to-end ETL pipeline using **Apache Airflow** for orchestration, **PySpark** for data processing, and **AWS DynamoDB** for storing processed results.  
It also includes sample CSV datasets and scripts for local development.

---

## ğŸ“Š Architecture

![Architecture](Airflow-Spark-Dynamo/architecture/Screenshot%20from%202025-08-30%2014-49-04.png)

---

## ğŸ“‚ Project Structure

Airflow-Spark-Dynamo
â”‚
â”œâ”€â”€ architecture
â”‚ â”œâ”€â”€ etl_using_airflow_pyspark.pdf # Detailed PDF documentation
â”‚ â””â”€â”€ Screenshot...png # Architecture diagram
â”‚
â”œâ”€â”€ dag-glue-workflow.py # Airflow DAG for orchestration
â”œâ”€â”€ data
â”‚ â”œâ”€â”€ songs.csv
â”‚ â”œâ”€â”€ streams1.csv
â”‚ â”œâ”€â”€ streams2.csv
â”‚ â””â”€â”€ users.csv
â”‚
â”œâ”€â”€ glue-dynamo.py # Loads data into DynamoDB
â”œâ”€â”€ glue-pyspark.py # PySpark ETL transformations
â””â”€â”€ local-docker-development.sh # Local Docker setup script


---

## âš™ï¸ Tech Stack
- **Apache Airflow** â€“ Orchestration  
- **Apache Spark (PySpark)** â€“ Data Transformation  
- **AWS DynamoDB** â€“ Storage  
- **Docker** â€“ Local Development  

---

## ğŸš€ How to Run
1. Clone the repository:
   ```bash
   https://github.com/Goku6382798/Airflow-Spark-Dynamo/tree/main
   cd Airflow-Spark-Dynamo
